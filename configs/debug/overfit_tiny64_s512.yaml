project_name: vpmoe-overfit-tiny64-s512
model: veerpareek/vpmoe-20b-init
trust_remote_code: true

# Keep this short so teacher+student can co-reside on a single GPU for debugging.
sequence_length: 512
use_flash_attention: true
model_kwargs:
  use_cache: false
  # Disable kernels/megablocks during debugging; re-enable later for perf.
  use_kernels: false

output_path: /data/distill_runs/overfit_tiny64_s512

dataset:
  train_dataset:
    disk_path: /data/distillation_debug/overfit_tiny64_s512
    split: train
  eval_dataset:
    disk_path: /data/distillation_debug/overfit_tiny64_s512
    split: validation
  seed: 42
  num_eval_samples: 16

# Start with logits-only distillation (CE + KL). Hidden-state distillation adds
# additional moving parts; enable after this passes.
loss_functions:
  - function: cross_entropy
    weight: 0.5
  - function: kl
    weight: 0.5
    temperature: 2.0

teacher:
  kind: hf
  path: openai/gpt-oss-20b
  kwargs:
    attn_implementation: flash_attention_2
    torch_dtype: bfloat16
    use_cache: false
    device_map: "cuda"

training_args:
  dataset_kwargs:
    skip_prepare_dataset: true
  packing: false
  max_steps: 2000
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 8

  dataloader_num_workers: 2
  dataloader_pin_memory: true
  per_device_eval_batch_size: 1
  eval_steps: 50
  save_steps: 200
  save_total_limit: 2
  logging_steps: 1

  # Muon (2D params) + AdamW (non-2D) composite optimizer.
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_ratio: 0.0
  lr_scheduler_type: constant

  bf16: true
  tf32: true
  max_grad_norm: 1.0
  optim: adamw_torch
  optim_args: "muon,lr=0.02,wd=0.1,update_scale=0.2,momentum=0.95,nesterov=true,ns_steps=5,adamw_lr=3e-4,adamw_wd=0.1"

  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  report_to: none

# Match the phase-1 plan: train attention only.
frozen_res:
  - ^model\.embed_tokens
  - ^lm_head
  - ^model\.norm\.weight$
  - ^model\.layers\.\d+\.input_layernorm\.weight$
  - ^model\.layers\.\d+\.post_attention_layernorm\.weight$
  - ^model\.layers\.\d+\.mlp\.


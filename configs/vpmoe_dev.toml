[meta]
# Local debug config for smoke tests on small dev GPUs.
# Not a spec change; do not use for real training.

[tokenizer]
vocab_size = 201088

[model]
num_layers = 8
hidden_size = 1024
num_attention_heads = 8
num_query_groups = 2
head_dim = 128
ffn_hidden_size = 128

value_residual = true
value_residual_init = 1.0

normalization = "RMSNorm"
qk_layernorm = true
softmax_type = "learnable"

# GPT-OSS parity (keep enabled even in debug).
add_bias_linear = true
add_qkv_bias = true

gated_linear_unit = false
activation = "squaredrelu"

untied_embeddings = true

[attention]
attention_dropout = 0.0
max_sequence_length = 2048
position_embedding_type = "grapem"

# 3:1 local:global schedule (same as baseline).
window_size = 128
window_attn_skip_freq = 4

[tpa]
use_tpa = true
tpa_rank = 2
tpa_q_rank = 4

[grape]
grape_a = true
grapem_learnable_freq = true
grapem_share_across_heads = true
grapem_log_freq_scale = 16.0

[moe]
num_experts = 16
topk = 2
shared_expert = true
shared_expert_size = 256

# First layer dense FFN, remaining layers MoE.
first_layer_dense = true

# DeepSeek-V3 router (group-limited + seq_aux_loss + expert bias).
router_load_balancing_type = "seq_aux_loss"
aux_loss_coeff = 1e-4
router_score_function = "sigmoid"
router_topk_scaling_factor = 2.5
router_num_groups = 2
router_group_topk = 1
router_enable_expert_bias = true
router_bias_update_rate = 1e-3
router_dtype = "fp32"

# Perf toggles (TransformerEngine-dependent).
grouped_gemm = false
permute_fusion = false
router_fusion = false

[optimizer]
name = "normuon"
normuon_momentum = 0.95
normuon_beta2 = 0.95
normuon_eps = 1e-10
polar_express_safety_factor = 2e-2

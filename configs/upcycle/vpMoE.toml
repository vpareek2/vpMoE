# Training config for vpMoE (80-layer MoE, real student attention/positional).

[job]
id = "vpMoE"
kind = "train_megatron"

[model]
object = "vpMoE"
mode = "real"

[tokenizer]
type = "O200kHarmonyTokenizer"
vocab_size_padded = 201088

[attention]
attention_dropout = 0.0
use_rope = false
use_grape = true
local_global_schedule = "3_to_1"
use_tpa = true
window_size = 128

[io]
load = "weights/upcycle/vpMoE"
save = "weights/upcycle/vpMoE"

# Upcycling object registry
#
# This file defines the canonical set of named checkpoint objects used by the
# Qwen3‑0.6B → vpDense0-5_28 → vpDense → vpMoE upcycling path.
#
# It is intentionally minimal and machine-checkable: later tools should validate
# produced checkpoints against these invariants and fail fast on mismatch.

[paths]
# All produced artifacts must live under the repo tree (no /opt-style roots).
weights_root = "weights"
# All produced upcycling outputs live under weights (repo-local, gitignored).
upcycle_root = "weights/upcycle"

[[objects]]
id = "qwen3-0_6B"
kind = "donor"
format = "hf_safetensors_single"
local_path = "weights/qwen3-0_6B"
provenance = "configs/upcycle/provenance/qwen3-0_6B.json"

[objects.model]
model_type = "qwen3"
num_layers = 28
hidden_size = 1024
num_attention_heads = 16
num_key_value_heads = 8
head_dim = 128
intermediate_size = 3072
vocab_size = 151936
tie_word_embeddings = true

[[objects]]
id = "gpt-oss-20b"
kind = "donor_vocab_reference"
format = "hf_safetensors"
local_path = "weights/gpt-oss-20b"
provenance = "configs/upcycle/provenance/gpt-oss-20b.json"

[objects.model]
tokenizer_family = "o200k_harmony"
vocab_size = 201088

[[objects]]
id = "qwen3-0_6B-o200k"
kind = "converted"
format = "hf_safetensors"
inputs = ["qwen3-0_6B", "gpt-oss-20b"]
outputs_under = "weights/upcycle/qwen3-0_6B-o200k"
config = "configs/upcycle/qwen3-0_6B-o200k.toml"

[objects.model]
model_type = "qwen3"
num_layers = 28
hidden_size = 1024
num_attention_heads = 16
num_key_value_heads = 8
head_dim = 128
intermediate_size = 3072
vocab_size = 201088
tie_word_embeddings = true
tokenizer_family = "o200k_harmony"

[[objects]]
id = "vpDense0-5_28"
kind = "student_dense"
format = "megatron_checkpoint"
inputs = ["qwen3-0_6B-o200k"]
outputs_under = "weights/upcycle/vpDense0-5_28"
config_convert = "configs/upcycle/vpDense0-5_28.convert.toml"
config_train_compat = "configs/upcycle/vpDense0-5_28.compat.toml"
config_train_real = "configs/upcycle/vpDense0-5_28.real.toml"

[objects.model]
num_layers = 28
hidden_size = 1024
num_attention_heads = 8
num_key_value_heads = 2
head_dim = 128
vocab_size = 201088
tokenizer_family = "o200k_harmony"
untied_embeddings = true
ffn_type = "dense"
# Dense FFN width for vpDense0-5_28. Chosen to match the vpMoE shared expert size
# so dense→MoE upcycling has a clean mapping.
intermediate_size = 512

[[objects]]
id = "vpDense"
kind = "student_dense"
format = "megatron_checkpoint"
inputs = ["vpDense0-5_28"]
outputs_under = "weights/upcycle/vpDense"
config_expand = "configs/upcycle/vpDense.expand.toml"
config_train = "configs/upcycle/vpDense.toml"

[objects.model]
num_layers = 80
hidden_size = 1024
num_attention_heads = 8
num_key_value_heads = 2
head_dim = 128
vocab_size = 201088
tokenizer_family = "o200k_harmony"
untied_embeddings = true
ffn_type = "dense"
# Dense FFN width for vpDense. Chosen to match the vpMoE shared expert size.
intermediate_size = 512

[[objects]]
id = "vpMoE"
kind = "student_moe"
format = "megatron_checkpoint"
inputs = ["vpDense"]
outputs_under = "weights/upcycle/vpMoE"
config_upcycle = "configs/upcycle/vpMoE.upcycle.toml"
config_train = "configs/upcycle/vpMoE.toml"

[objects.model]
num_layers = 80
hidden_size = 1024
num_attention_heads = 8
num_key_value_heads = 2
head_dim = 128
vocab_size = 201088
tokenizer_family = "o200k_harmony"
untied_embeddings = true
ffn_type = "moe"
num_experts = 256
topk = 4
shared_expert = true
shared_expert_size = 512
intermediate_size = "TBD_locked_student_I_target"

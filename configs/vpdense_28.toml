[meta]
# vpDense0-5_28: 28-layer dense warmstart target for Qwen3â†’student weight surgery.
#
# This is a *dense* (non-MoE) architectural reference: it matches the locked student
# tokenizer + attention/positional stack, but uses a dense FFN sized to map cleanly
# into the vpMoE shared expert during upcycling.
#
# Canonical upcycling runbook: `vpmoe/upcycle/README.md`.
#
# NOTE: Training upcycling uses Megatron flags (e.g. `--num-experts` / `--moe-ffn-hidden-size`);
# this file intentionally does not encode MoE routing settings.

[tokenizer]
vocab_size = 201088

[model]
num_layers = 28
hidden_size = 1024
num_attention_heads = 8
num_query_groups = 2
head_dim = 128

# Dense FFN width (chosen to match the vpMoE shared expert size).
ffn_hidden_size = 512

normalization = "RMSNorm"
qk_layernorm = true
softmax_type = "learnable"

add_bias_linear = true
add_qkv_bias = false

gated_linear_unit = false
activation = "squaredrelu"

untied_embeddings = true

[attention]
max_sequence_length = 4096
position_embedding_type = "grapem"

# Real student stack: 3:1 local:global schedule with windowed local attention.
window_size = 128
window_attn_skip_freq = 4

[tpa]
use_tpa = true
tpa_rank = 2
tpa_q_rank = 4

[grape]
grape_a = true
grapem_learnable_freq = true
grapem_share_across_heads = true
grapem_log_freq_scale = 16.0

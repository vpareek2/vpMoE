[meta]
# Baseline vpMoE config (locked reference). The repo default lives in `configs/vpmoe.toml`.

[tokenizer]
# o200k Harmony, padded vocab sizes
vocab_size = 201088

[model]
num_layers = 80
hidden_size = 1024
num_attention_heads = 8
num_query_groups = 2
head_dim = 128

# Routed expert FFN width.
ffn_hidden_size = 128

normalization = "RMSNorm"
qk_layernorm = true
softmax_type = "learnable"

add_bias_linear = true
add_qkv_bias = false

# If you switch to a gated MLP (e.g. SwiGLU), update ffn_hidden_size accordingly.
gated_linear_unit = false
activation = "squaredrelu"

# Untie embeddings for the student.
untied_embeddings = true

[attention]
max_sequence_length = 4096
position_embedding_type = "grapem"

# 3:1 local:global schedule => every 4th layer is global (full attention).
window_size = 128
window_attn_skip_freq = 4

[tpa]
use_tpa = true
# TPA ranks affect parameter count; keep these pinned for reproducibility.
tpa_rank = 2
tpa_q_rank = 4

[grape]
grape_a = true
grapem_learnable_freq = true
grapem_share_across_heads = true
grapem_log_freq_scale = 16.0

[moe]
num_experts = 256
topk = 4
shared_expert = true
shared_expert_size = 512

# First layer dense FFN, remaining layers MoE.
first_layer_dense = true

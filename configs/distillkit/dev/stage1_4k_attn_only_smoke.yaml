project_name: vpmoe-stage1-4k-attn-only-smoke
trust_remote_code: true
model: /workspace/vpmoe/weights/vpmoe-20b-init
output_path: /data/distill_runs/vpmoe_stage1_4k_attn_only_smoke
use_flash_attention: true
sequence_length: 4096
resize_embeddings_to_multiple_of: 201088

dataset:
  train_dataset:
    disk_path: /data/distillation_1/phase1_mix_4k_665m
    split: train
  eval_dataset:
    disk_path: /data/distillation_1/phase1_mix_4k_665m
    split: validation
  seed: 12
  num_samples: 128
  num_eval_samples: 32

loss_functions:
  - function: cross_entropy
    weight: 0.2
  - function: kl
    weight: 0.2
    temperature: 2.0
  - function: hs_cosine
    weight: 0.6

layer_mapping: all

teacher:
  kind: hf
  path: /data/gpt-oss-20b-bf16
  kwargs:
    attn_implementation: flash_attention_2
    torch_dtype: bfloat16
    device_map: cuda:0

training_args:
  dataset_kwargs:
    skip_prepare_dataset: true
  packing: false
  max_steps: 10
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 2
  save_steps: 5
  save_total_limit: 1
  logging_steps: 1
  eval_steps: 5
  per_device_eval_batch_size: 1
  learning_rate: 1.0e-3
  weight_decay: 0.0
  warmup_ratio: 0.025
  lr_scheduler_type: linear
  bf16: true
  max_grad_norm: 0.5
  optim: adamw_torch
  optim_args: "muon"
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false
  report_to: none

frozen_res:
  - ^model\.embed_tokens
  - ^lm_head
  - ^model\.norm
  - ^model\.layers\.\d+\.mlp

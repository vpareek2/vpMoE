project_name: vpmoe-stage1-4k-smoke-2xh100
model: veerpareek/vpmoe-20b-init
trust_remote_code: true
model_kwargs:
    use_cache: false
    use_kernels: true

frozen_res: # regex param names for frozen weights
    - ^model\.embed_tokens
    - ^lm_head
    - ^model\.norm\.weight$
    - ^model\.layers\.\d+\.input_layernorm\.weight$
    - ^model\.layers\.\d+\.post_attention_layernorm\.weight$
    - ^model\.layers\.\d+\.mlp\.

output_path: /data/distill_runs/vpmoe_stage1_4k_smoke_2xh100
use_flash_attention: true
sequence_length: 4096

dataset:
    train_dataset:
        disk_path: /data/distillation_1/phase1_mix_4k_665m
        split: train
    eval_dataset:
        disk_path: /data/distillation_1/phase1_mix_4k_665m
        split: validation
    seed: 42
    num_eval_samples: 64

loss_functions:
    - function: cross_entropy
      weight: 0.2
    - function: kl
      weight: 0.2
      temperature: 2.0
    - function: hs_cosine # cosine loss between hidden states
      weight: 0.6

layer_mapping: all

teacher:
    kind: hf
    path: openai/gpt-oss-20b
    kwargs:
        attn_implementation: kernels-community/vllm-flash-attn3
        torch_dtype: bfloat16
        use_cache: false

training_args:
    dataset_kwargs:
        skip_prepare_dataset: true
    packing: false
    num_train_epochs: 1
    max_steps: 200
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 1

    dataloader_num_workers: 2
    dataloader_pin_memory: true
    eval_steps: 50
    per_device_eval_batch_size: 1
    save_steps: 100
    save_total_limit: 1
    logging_steps: 1

    learning_rate: 1.0e-3
    weight_decay: 0.00
    warmup_ratio: 0.025
    lr_scheduler_type: cosine_with_min_lr
    lr_scheduler_kwargs:
        min_lr: 1.0e-5

    bf16: true
    tf32: true
    max_grad_norm: 0.5
    optim: adamw_torch
    optim_args: "muon"

    gradient_checkpointing: true
    gradient_checkpointing_kwargs:
        use_reentrant: false
